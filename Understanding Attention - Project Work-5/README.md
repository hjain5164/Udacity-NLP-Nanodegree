## Introduction to the Project
In this notebook, we look at how attention is implemented. The focus was on implementing attention in isolation from a larger model. That's because when implementing attention in a real-world model, a lot of the focus goes into piping the data and juggling the various vectors rather than the concepts of attention themselves.

In this notebook I have implemented attention scoring as well as calculating an attention context vector.

## Installation
You require following modules for run the [notebook](https://github.com/hjain5164/Udacity-NLP-Nanodegree/blob/master/Understanding%20Attention%20-%20Project%20Work-5/%5BSOLUTION%5D%20Attention%20Basics.ipynb):
* numpy
* matplotlib
